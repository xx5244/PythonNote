# 爬蟲基礎知識
---

+ ## Robots協議
    **robots.txt（統一小寫）是一種存放於網站根目錄下的ASCII編碼的文字檔案，它通常告訴網路搜尋引擎的漫遊器（又稱網路蜘蛛），此網站中的哪些內容是不應被搜尋引擎的漫遊器取得的，哪些是可以被漫遊器取得的。因為一些系統中的URL是大小寫敏感的，所以robots.txt的檔名應統一為小寫。**
    + ### 範例
       [google](https://www.google.com.tw/robots.txt)
       [百度](http://baidu.com/robots.txt)

+ ## 請求與響應
    **網路溝通的組成是Client與Server溝通的過程**
    1. 當我們瀏覽器輸入網址，等同客戶端向伺服器端發送請求(Request)去獲取該網址的HTML文件
    2. 伺服器會把Response文件發回給瀏覽器
    3. 瀏覽器解析Response中的HTML文件
    4. 若其中有引用別的文件，如Image文件、CSS文件、JS文件...等，瀏覽器會再次發送請求給伺服器端以獲取對應的文件
    5. 所有的文件都成功請求到之後，網頁會根據HTML語法結構，完整顯示出來

+ ## Chrome開發者工具
    **超常用的工具沒有之一**
    1. ### 元素面板(Elements)
        **通過元素面板，能查看頁面渲染內容所在的標籤、CSS屬性...等屬性**
    ![](../Image/elements.PNG)

    2. ### 控制台(Console)
        **是用於顯示JS和DOM物件訊息的視窗**
        **主要是用來調試運行JS程式碼**
    ![](../Image/Console.PNG)

    3. ### 資源面板(Sources)
        **可察看當前網頁的原始文件(source file)**
        **左側為原始文件，以樹狀結構方式顯示**
        **中側為可調試JS代碼的區域**
        **右側為斷點調試功能區域**
    ![](../Image/Sources.PNG)

    4. ### 網路面板(Network)
        **很常用的功能**
        **該面板記錄著每個網路操作的相關訊息**
        **包括詳細的耗時數據、HTTP請求及響應標頭、Cookie...等**
        ![](../Image/Network.PNG)
       + 工具欄
       + Stop recoding network log
        **停止/開啟 記錄所有的網路請求**
        **預設為開啟，紅色為開啟，灰色為關閉**
        ![](../Image/recoding_network_log.PNG)
        ![](../Image/Stop_recoding_network_log.PNG)
       + Clear
           **清空所有的數據**
        ![](../Image/Clear.PNG)
       + Filter
           **數據過濾器**
           **藍色為開啟、灰色為關閉**
        ![](../Image/Filter_open.PNG)
        ![](../Image/Filter_close.PNG)
         + Requests Table
            **此表格會列出向伺服器索取的每一個HTTP請求，默認情況是按時間排序**
            ![](../Image/Requests_table.PNG)
         + all(所有請求數據)
         + XHR(XML HttpRequest的縮寫，是ajax技術的核心，動態加載完成經常分析的一個內容)
         + CSS(css樣式的文件)
         + JS(JavaScript文件，js解密是經常分析的頁面)
         + Img(圖片文件)
         + Font(字體文件)
         + DOC(Document，文檔內容)
         + WS(WebSocket,web端的Socket數據通信，一般用於一些實時更新的數據)
         + Manifest(顯示通過Manifest緩存的資源，包含很多信息，如js資料文件會顯示文件地址、大小、類型)
       + Search
          **搜尋欄，只要在all裡面出現過的內容，就可以被直接搜索到**
       + Preserve log
        **保留日誌，做爬蟲一定要勾選上**
       + Disable cache
        **清空Javascript、CSS文件暫存，獲取最新的**
       + Hide data URLs
        **用於是否隱藏dataurl，Data URL技術是圖片數據以base64字串格式嵌入到頁面中和HTML融為一體**

    5. ### Requests詳情
    ![](../Image/Requests詳情.PNG)
       + #### Headers
         **顯示HTTP請求的標頭檔**
         1. General
            1. Request url:實際請求的網址
            2. Request Method:請求的方法
            3. Status code:狀態碼
           ![](../Image/Headers_General.PNG)
         2. Response Headers
             **伺服器返回時設置的一些數據，例如:伺服器更新的cookieg數據為新是在這裡出現修改**
           ![](../Image/Headers_Response.PNG)
         3. Requests Headers
             **請求標頭檔，請求不到數據的原因一般出現在這裡**
            1. Accept:伺服器接收的數據格式(一般忽略)
            2. Accept-Encoding:伺服器接收的編碼格式(一般忽略)
            3. Accept-Language:伺服器接收的語言(一般忽略)
            4. Connection:保持連接(一般忽略)
            5. Cookies:是身分訊息，爬取VIP資源是需要此訊息的
            6. Host:請求的主機位址
            7. User-Agent:用戶身分代理，伺服器用此訊息判斷用戶
            8. Sec-xxx-xxx:其他訊息
           ![](../Image/Headers_Requests.PNG)

    6. ### Preview
        **請求結果的預覽，一般用來查看請求到的圖片**
      ![](../Image/Preview.PNG)
    7. ### Response
        **請求返回的結果，一般的內容是整個網站的源代碼，如果是非同步請求，返回的結果一般是Json文本數據**
        **此數據與瀏覽器顯示的網頁可能不一致，因為瀏覽器是動態加載的**
      ![](../Image/Response.PNG)

+ ## 爬蟲思路
  1. 欲爬取資料的網站網址(url)
     1. 靜態網頁:直接是網址欄內的網址
     2. 動態網頁:通過開發者工具抓包
  2. 請求url的數據
     1. 文本數據
     2. js數據
     3. css數據
     4. 圖片數據
     5. 影片數據
     6. etc
  3. 數據解析，解析出自己想要的數據
     1. css選擇器
     2. 正則表達式
     3. Xpath節點提取
  4. 數據保存
     1. 本機
     2. 資料庫
        1. redis
        2. MongoDB
     3. 雲端

+ ## HTTP/HTTPS
  + HTTP(HyperText Transfer Protocol)超文本傳輸協定:是一個客戶端（使用者）和伺服器端（網站）之間請求和應答的標準，通常使用TCP協定。HTTP是全球資訊網的數據通信的基礎。**
  + HTTPS(HyperText Transfer Protocol Secure)超文本傳輸安全協定:簡單的來說就是HTTP的安全版，在HTTP下加入SSL層
  + HTTP的缺點:
    1. 通信是用明文(不加密)，內容可能被竊聽
    2. 不會驗證通信方的身分，有可能遭遇偽裝
    3. 無法證明資訊的完整性，有可能遭到竄改
  + HTTPS的優點:
    解決上述問題，在HTTP層以下加入SSL或TSL層，主要用途為對連接的訊息加密
    ![](../Image/HTTP_HTTPS.png)

+ ## URL
  + URL(Uniform Resource Locator)統一資源定位符:俗稱:網址，是網際網路上標準的資源的位址。
  + 統一資源定位符的`標準`格式如下：
  ```
  [協定類型]://[伺服器位址]:[埠號]/[資源層級UNIX檔案路徑][檔名]?[查詢]#[片段ID]
  ```
  + 統一資源定位符的`完整`格式如下：
  ```
  [協定類型]://[存取資源需要的憑證資訊]@[伺服器位址]:[埠號]/[資源層級UNIX檔案路徑][檔名]?[查詢]#[片段ID]
  ```
  PS:其中[存取憑證資訊]、[埠號]、[查詢]、[片段ID]都屬於選填項。
  + 範例:
    ```
    https://zh.wikipedia.org:443/w/index.php?title=隨機頁面
    ```
    1. https:協定類型
    2. zh.wikipedia.org:伺服器位址
    3. 443:埠號
    4. /w/index.php:資源層級UNIX檔案路徑
    5. ?title=隨機頁面:查詢

+ ## Request(請求資訊)
  + ### 組成
  **由以下4個組成**
    1. 請求行
    2. 請求頭
    3. 空行
    4. 請求體
    **注意:這是HTTP下的協議，與chrome上看到的Headers的資訊會有所差異**
    ![](../Image/Requests_Headers.png)
    ![](../Image/Requests_Headers_2.png)
  + ### 請求方法
    1. GET(極度常用):向指定的資源發出「`顯示`」請求
    2. POST(常用):向指定資源`提交資料`，請求伺服器進行處理（例如提交表單或者上傳檔案）
    3. HEAD:與GET方法一樣，都是向伺服器發出指定資源的請求。只不過伺服器將不傳回資源的本文部份。它的好處在於，使用這個方法可以在不必傳輸全部內容的情況下，就可以取得其中「關於該資源的訊息」（元訊息或稱元資料）
    4. PUT:向指定資源位置上傳其最新內容
    5. DELETE:請求伺服器刪除Request-URI所標識的資源
    6. TRACE:回顯伺服器收到的請求，主要用於測試或診斷
    7. OPTIONS:這個方法可使伺服器傳回該資源所支援的所有HTTP請求方法。用'*'來代替資源名稱，向Web伺服器傳送OPTIONS請求，可以測試伺服器功能是否正常運作
    8. CONNECT:HTTP/1.1協定中預留給能夠將連接改為隧道方式的代理伺服器。通常用於SSL加密伺服器的連結（經由非加密的HTTP代理伺服器）
    PS:Get與Post方法區別
    1. Get是向伺服器獲取數據，而Post則是向伺服器傳遞數據
    2. Get的請求參數都顯示在網址上面
    3. Post請求參數在請求體當中，沒有長度限制並以隱藏的方式傳送，通常用來向HTTP伺服器提交量比較大的數據。 請求的參數類型包含在`Content-Type`裡面，指名發送請求時，提交內容的數據格式
  + ### 常用的請求頭資訊
    1. User-Agent:瀏覽器的瀏覽器身分標識字串
    2. Referer:表示瀏覽器所存取的前一個頁面，正是那個頁面上的某個連結將瀏覽器帶到了當前所請求的這個頁面
    3. Host:伺服器的域名(用於虛擬主機 )，以及伺服器所監聽的傳輸控制協定埠號。如果所請求的埠是對應的服務的標準埠，則埠號可被省略
    4. Origin:聲明請求資源的起始位置
    5. Cookie:之前由伺服器通過Set-Cookie傳送的一個超文字傳輸協定Cookie
    6. Accept:能夠接受的回應內容類型（Content-Types）
    7. Accept-Charset:能夠接受的字元集 
    8. Accept-Encoding:能夠接受的編碼方式列表
    9. Accept-Language:能夠接受的回應內容的自然語言列表
    10. Authorization:用於超文字傳輸協定的認證的認證資訊
    11. Content-Length:請求體的長度，以8位元的位元組表示
    12. Connection:處理完這次請求後，是斷開連結還是繼續保持連結
    13. From:發起此請求的使用者的郵件位址
    14. Upgrade:要求伺服器升級到另一個協定
    15. Upgrade-Insecure-Requests:要求伺服器升級至https，並在往後請求都使用https

  + ### 範例
    ```python
    import requests

    url = 'https://www.google.com.tw/'
    response = requests.get(url)

    print(f'{"*"*5}以下是headers{"*"*5}')
    print(response.request.headers)
    print(f'{"*"*5}以下是url{"*"*5}')
    print(response.request.url)
    print(f'{"*"*5}以下是method{"*"*5}')
    print(response.request.method)
    ```

+ ## Response(響應資訊)
  + ### 組成
  **由以下4個組成**
    1. 響應行
    2. 響應頭
    3. 空行
    4. 響應體
    ![](../Image/Response_Headers.jpg)
    ![](../Image/Response_Headers_2.png)
  + ### 響應行
    **協議版本 + 狀態碼 + 狀態碼描述**
  + ### 響應頭
    **用於描述伺服器的基本訊息及數據描述**
  + ### 響應體
    **響應的內容，如請求的是HTML頁面，返回就是HTML代碼，若是請求JS頁面即返回JS代碼**
    **常見的內容有:HTML、CSS、JS、圖片、影片、音頻**
  + ### 常見響應方法
    1. text:文本數據
    2. content:二進制數據
    3. json():json數據
    4. headers:響應頭訊息
    5. encoding:響應體的編碼
    6. apparent_encoding:自動辨識響應體的編碼
    7. cookies:cookie訊息
    8. url:響應體的url
    9. status_code:狀態碼
  + ### 常用狀態碼
    1. 200 OK:請求正常
    2. 301 Moved Permanently:被請求的資源已永久移動到新位置，並且將來任何對此資源的參照都應該使用本回應返回的若干個URI之一
    3. 307 Temporary Redirect:臨時的重新導向
    4. 400 Bad Request:由於明顯的客戶端錯誤，伺服器不能或不會處理該請求
    5. 401 Unauthorized:類似於403 Forbidden，401語意即「未認證」，即使用者沒有必要的憑據
    6. 403 Forbidden:伺服器已經理解請求，但是拒絕執行它，與401回應不同的是，身分驗證並不能提供任何幫助，而且這個請求也不應該被重複提交
    7. 404 Not Found:請求失敗，請求所希望得到的資源未被在伺服器上發現，但允許使用者的後續請求
    8. 502 Bad Gateway:作為閘道器或者代理工作的伺服器嘗試執行請求時，從上游伺服器接收到無效的回應







